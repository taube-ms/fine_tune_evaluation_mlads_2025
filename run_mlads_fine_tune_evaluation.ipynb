{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating LLM and SLM with Fine-Tuning: A Live Coding Tutorial for Evaluating Accuracy, Latency, and Cost To Find The Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daniel Taube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, pipeline\n",
    "import evaluate\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive review is:\n",
      "'The Luzhin Defence' is a movie worthy of anyone's time. it is a brooding, intense film, and kept my attention the entire time. John Turturro is absolutely stunning in his portrayal of a tender, eccentric chess Grandmaster, and Emily Watson is spell-binding as the gentle but rebellious daughter of a highly respected Russian family. The chemistry between Watson and Turturro on screen is obvious from the moment their characters meet in the story. All in all, this movie is one of the best in-depth looks at the life of a chess Grandmaster, and Turturro and Watson add a whole non-mainstream, non-cliche feel to the film. Most people will come out of the theater thinking, and feeling somewhat touched by this brilliant look at the most unlikely of love stories.\n",
      "A Negative review is:\n",
      "This flick was a blow to me. I guess little girls should aspire to be nothing more than swimsuit models, home makers or mistresses, since that seems to be all they'll ever be portrayed as anyway. It is truly saddening to see an artist's work and life being so unjustly misinterpretated. Inconcievably (or perhaps it should have been expected), Artemisia's entire character and all that she stands for, had been reduced to a standard Hollywood, female character; a pitiful, physically flawless, helpless little creature, displaying none of the character traits that actually got her that place in history which was being mutilated here. Sadder yet, was to see that a great part of the audience was too badly educated in the area to comprehend the incredible gap between the message conveyed in the film, and reality. To portray the artist as someone in love with her real-life rapist, someone whom she in reality accused of raping her even when under torture, just plain pisses me off. If the director had nothing more substantial to say she should have refrained from basing her story on a real person.\n"
     ]
    }
   ],
   "source": [
    "random_int_between_0_and_1 = random.randint(0, 12000)\n",
    "\n",
    "# pretty print the review\n",
    "print(\"A positive review is:\")\n",
    "print(df[df.label == 1].iloc[random_int_between_0_and_1].text)\n",
    "print(\"A Negative review is:\")\n",
    "print(df[df.label == 0].iloc[random_int_between_0_and_1].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random 1000 positive and 1000 negative reviews\n",
    "positive_reviews = df[df.label == 1].sample(1000)\n",
    "negative_reviews = df[df.label == 0].sample(1000)\n",
    "# combine the two dataframes\n",
    "df_small = pd.concat([positive_reviews, negative_reviews])\n",
    "# shuffle the dataframe\n",
    "df_small = df_small.sample(frac=1).reset_index(drop=True)\n",
    "# save the dataframe to a csv file\n",
    "df_small.to_csv(\"imdb_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random 1000 positive and 1000 negative reviews\n",
    "positive_reviews = df_test[df_test.label == 1].sample(1000)\n",
    "negative_reviews = df_test[df_test.label == 0].sample(1000)\n",
    "# combine the two dataframes\n",
    "df_small_test = pd.concat([positive_reviews, negative_reviews])\n",
    "# shuffle the dataframe\n",
    "df_small_test = df_small_test.sample(frac=1).reset_index(drop=True)\n",
    "# save the dataframe to a csv file\n",
    "df_small_test.to_csv(\"imdb_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2000 examples [00:00, 16785.61 examples/s]\n",
      "Generating test split: 2000 examples [00:00, 41275.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV datasets (ensure your train.csv and test.csv are in the working directory)\n",
    "data_files = {\"train\": \"imdb_train.csv\", \"test\": \"imdb_test.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Load a pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\") # ~66M params, SLM\n",
    "\n",
    "# 4B params, SLM, can run on 1 GPU in inference\n",
    "# 70-300B params, LLM, Need a cluster of GPUs to run in inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2000/2000 [00:07<00:00, 257.63 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:07<00:00, 271.44 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\danieltaube\\Documents\\mlads\\mlads_2025\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function for the 'text' field in your CSV\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove the original text column (optional) and set the format to PyTorch tensors\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# Use a data collator that dynamically pads the inputs\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Load a small pretrained model for sequence classification with 2 output labels (0 and 1)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hi, welcome to the world of NLP! This is a test sentence for the DistilBERT model. Let's see how well it performs on this text.\"\"\"\n",
    "\n",
    "# toknizer into a tensor pt\n",
    "# 1. Tokenize the text\n",
    "\n",
    "\n",
    "inputs_ids = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128, return_tensors='pt')['input_ids']\n",
    "\n",
    "# use the model to embed the text\n",
    "outputs = model.distilbert.embeddings(inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danieltaube\\AppData\\Local\\Temp\\ipykernel_26368\\250823425.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Load an accuracy metric for evaluation\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.5025317668914795, 'eval_accuracy': 0.8445, 'eval_runtime': 75.2018, 'eval_samples_per_second': 26.595, 'eval_steps_per_second': 3.324, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"distilbert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9864317774772644}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9901705384254456}]\n"
     ]
    }
   ],
   "source": [
    "# run the model on a sample text\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-imdb\")\n",
    "print(classifier(\"This movie was fantastic! I loved it!\"))\n",
    "print(classifier(\"This movie was terrible! I hated it!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running distilbert-base-uncased on a sample text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5127051472663879}]\n",
      "[{'label': 'LABEL_0', 'score': 0.5029503107070923}]\n",
      "     \n",
      "Running distilbert-base-uncased-imdb on a sample text\n",
      "[{'label': 'LABEL_1', 'score': 0.9864317774772644}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9901705384254456}]\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "# run the model on a sample text\n",
    "def run_example_model(model_name, prompt = ''):\n",
    "    print(f\"Running {model_name} on a sample text\")\n",
    "    classifier = pipeline(\"text-classification\", model=model_name)\n",
    "    print(classifier(prompt + \"This movie was fantastic! I loved it!\"))\n",
    "    print(classifier(prompt + \"This movie was terrible! I hated it!\"))\n",
    "    print(\"     \")\n",
    "\n",
    "run_example_model(\"distilbert-base-uncased\")\n",
    "run_example_model(\"distilbert-base-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_slm(model_name, train_path, test_path):\n",
    "    # Load the CSV datasets (ensure your train.csv and test.csv are in the working directory)\n",
    "    data_files = {\"train\": train_path, \"test\": test_path}\n",
    "    dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "    # Load a pretrained tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenization function for the 'text' field in your CSV\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # Remove the original text column (optional) and set the format to PyTorch tensors\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    # Use a data collator that dynamically pads the inputs\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Load a small pretrained model for sequence classification with 2 output labels (0 and 1)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Load an accuracy metric for evaluation\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(model_name + \"-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of larger models with approximate parameter counts:\n",
    "bigger_models = [\n",
    "    \"bert-large-uncased\",          # ~340 million parameters - 5 times larger than DistilBERT\n",
    "]\n",
    "\n",
    "for model_name in bigger_models:\n",
    "    fine_tune_slm(model_name, \"imdb_train.csv\", \"imdb_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bert-large-uncased on a sample text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.679204523563385}]\n",
      "[{'label': 'LABEL_1', 'score': 0.7086663842201233}]\n",
      "     \n",
      "Running bert-large-uncased-imdb on a sample text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9978277087211609}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9881008863449097}]\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "run_example_model(\"bert-large-uncased\")\n",
    "run_example_model(\"bert-large-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add prompt to the untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "    Analyze the sentiment of the following movie review.\n",
    "    \n",
    "    Please classify the sentiment as:\n",
    "    - LABEL_1 if the review is positive\n",
    "    - LABEL_0 if the review is negative\n",
    "    \n",
    "    Return only the number (LABEL_1 or LABEL_0) without any additional text.\n",
    "\n",
    "    Review:\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running distilbert-base-uncased on a sample text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.5130700469017029}]\n",
      "[{'label': 'LABEL_1', 'score': 0.5139321684837341}]\n",
      "     \n",
      "Running distilbert-base-uncased-imdb on a sample text\n",
      "[{'label': 'LABEL_1', 'score': 0.9864317774772644}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9901705384254456}]\n",
      "     \n",
      "Running bert-large-uncased on a sample text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.851081907749176}]\n",
      "[{'label': 'LABEL_0', 'score': 0.8455356359481812}]\n",
      "     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running bert-large-uncased-imdb on a sample text\n",
      "[{'label': 'LABEL_1', 'score': 0.9978277087211609}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9881008863449097}]\n",
      "     \n"
     ]
    }
   ],
   "source": [
    "run_example_model(\"distilbert-base-uncased\", prompt = prompt)\n",
    "run_example_model(\"distilbert-base-uncased-imdb\")\n",
    "run_example_model(\"bert-large-uncased\", prompt = prompt)\n",
    "run_example_model(\"bert-large-uncased-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracy and latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Evaluating distilbert-base-uncased:   1%|          | 1/100 [00:00<00:19,  5.01it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Evaluating distilbert-base-uncased:   3%|▎         | 3/100 [00:00<00:08, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:   7%|▋         | 7/100 [00:00<00:07, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  14%|█▍        | 14/100 [00:01<00:07, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  18%|█▊        | 18/100 [00:01<00:04, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  40%|████      | 40/100 [00:03<00:05, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  45%|████▌     | 45/100 [00:03<00:03, 14.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  59%|█████▉    | 59/100 [00:05<00:03, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  67%|██████▋   | 67/100 [00:06<00:03, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  89%|████████▉ | 89/100 [00:08<00:00, 14.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased:  94%|█████████▍| 94/100 [00:08<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased: 100%|██████████| 100/100 [00:09<00:00, 10.97it/s]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased\n",
      "Using prompt: Yes\n",
      "Accuracy: 0.3900\n",
      "Total latency: 9.12 seconds\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:   0%|          | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Evaluating distilbert-base-uncased-imdb:   5%|▌         | 5/100 [00:00<00:05, 17.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (575) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (837) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:   9%|▉         | 9/100 [00:00<00:05, 15.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (602) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  20%|██        | 20/100 [00:01<00:03, 20.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (685) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (706) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (699) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (943) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  40%|████      | 40/100 [00:02<00:04, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (631) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (1176) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  45%|████▌     | 45/100 [00:03<00:03, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (595) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  61%|██████    | 61/100 [00:04<00:02, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (544) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  69%|██████▉   | 69/100 [00:05<00:02, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (740) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  90%|█████████ | 90/100 [00:06<00:00, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (831) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb:  95%|█████████▌| 95/100 [00:06<00:00, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (570) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating distilbert-base-uncased-imdb: 100%|██████████| 100/100 [00:07<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: distilbert-base-uncased-imdb\n",
      "Using prompt: No\n",
      "Accuracy: 0.7500\n",
      "Total latency: 7.24 seconds\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Evaluating bert-large-uncased:   1%|          | 1/100 [00:00<01:15,  1.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:   3%|▎         | 3/100 [00:01<00:33,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:   6%|▌         | 6/100 [00:02<00:50,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  12%|█▏        | 12/100 [00:05<00:44,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  14%|█▍        | 14/100 [00:06<00:39,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  16%|█▌        | 16/100 [00:07<00:31,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  38%|███▊      | 38/100 [00:19<00:33,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  40%|████      | 40/100 [00:20<00:30,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  42%|████▏     | 42/100 [00:21<00:27,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  58%|█████▊    | 58/100 [00:31<00:28,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  65%|██████▌   | 65/100 [00:34<00:19,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  67%|██████▋   | 67/100 [00:35<00:14,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  85%|████████▌ | 85/100 [00:45<00:09,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased:  92%|█████████▏| 92/100 [00:48<00:03,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (514) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased: 100%|██████████| 100/100 [00:52<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-large-uncased\n",
      "Using prompt: Yes\n",
      "Accuracy: 0.4300\n",
      "Total latency: 52.33 seconds\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Evaluating bert-large-uncased-imdb:   1%|          | 1/100 [00:00<01:19,  1.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (575) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:   3%|▎         | 3/100 [00:01<00:31,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (837) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:   6%|▌         | 6/100 [00:03<00:52,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (602) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  14%|█▍        | 14/100 [00:07<00:57,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (685) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  16%|█▌        | 16/100 [00:07<00:37,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (706) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (699) must match the size of tensor b (512) at non-singleton dimension 1\n",
      "Error processing example: The size of tensor a (943) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  38%|███▊      | 38/100 [00:17<00:26,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (631) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  40%|████      | 40/100 [00:18<00:24,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (1176) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  42%|████▏     | 42/100 [00:19<00:22,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (595) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  58%|█████▊    | 58/100 [00:27<00:26,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (544) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  67%|██████▋   | 67/100 [00:32<00:24,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (740) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  86%|████████▌ | 86/100 [00:43<00:09,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (831) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb:  92%|█████████▏| 92/100 [00:45<00:03,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing example: The size of tensor a (570) must match the size of tensor b (512) at non-singleton dimension 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating bert-large-uncased-imdb: 100%|██████████| 100/100 [00:48<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-large-uncased-imdb\n",
      "Using prompt: No\n",
      "Accuracy: 0.8000\n",
      "Total latency: 48.78 seconds\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_accuracy_latency(model_name, use_prompt=False, num_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate a model's accuracy and latency on the IMDB test set.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): Name of the model to evaluate\n",
    "        use_prompt (bool): Whether to add prompt to inputs\n",
    "        num_samples (int): Number of samples to evaluate (for faster testing)\n",
    "    \"\"\"\n",
    "    # Load test dataset\n",
    "    df_test = pd.read_csv(\"imdb_test.csv\").sample(num_samples, random_state=42)\n",
    "    \n",
    "    # Load model and create classifier\n",
    "    classifier = pipeline(\"text-classification\", model=model_name)\n",
    "    tokenizer = classifier.tokenizer\n",
    "    \n",
    "    # Prepare for evaluation\n",
    "    correct_predictions = 0\n",
    "    total_examples = len(df_test)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate max allowed review length to fit within context window if using prompt\n",
    "    max_model_length = tokenizer.model_max_length\n",
    "    prompt_length = len(tokenizer.encode(prompt)) - 2  # -2 for special tokens\n",
    "    max_review_length = max_model_length - prompt_length if use_prompt else max_model_length\n",
    "    \n",
    "    # Iterate through test examples\n",
    "    for _, row in tqdm(df_test.iterrows(), total=total_examples, desc=f\"Evaluating {model_name}\"):\n",
    "        text = row[\"text\"]\n",
    "        true_label = row[\"label\"]\n",
    "        \n",
    "        # Truncate text if needed\n",
    "        if use_prompt:\n",
    "            # Tokenize text to check length\n",
    "            tokens = tokenizer.encode(text)[1:-1]  # Remove special tokens\n",
    "            if len(tokens) > max_review_length:\n",
    "                tokens = tokens[:max_review_length]\n",
    "                text = tokenizer.decode(tokens)\n",
    "            \n",
    "            input_text = prompt + text\n",
    "        else:\n",
    "            input_text = text\n",
    "        \n",
    "        # Get prediction\n",
    "        try:\n",
    "            prediction = classifier(input_text)[0]\n",
    "            \n",
    "            # Extract the predicted label\n",
    "            if prediction['label'] == 'LABEL_1':\n",
    "                predicted_label = 1\n",
    "            else:\n",
    "                predicted_label = 0\n",
    "            \n",
    "            # Check if prediction is correct\n",
    "            if predicted_label == true_label:\n",
    "                correct_predictions += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate metrics\n",
    "    end_time = time.time()\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    latency = end_time - start_time\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Using prompt: {'Yes' if use_prompt else 'No'}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Total latency: {latency:.2f} seconds\")\n",
    "    print(\"------------------------------\")\n",
    "    \n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"use_prompt\": use_prompt,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"total_latency\": latency,\n",
    "    }\n",
    "\n",
    "def evaluate_all_models():\n",
    "    \"\"\"Run evaluation on all models and return results as DataFrame\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Models to evaluate\n",
    "    models = [\n",
    "        {\"name\": \"distilbert-base-uncased\", \"use_prompt\": True},\n",
    "        {\"name\": \"distilbert-base-uncased-imdb\", \"use_prompt\": False},\n",
    "        {\"name\": \"bert-large-uncased\", \"use_prompt\": True},\n",
    "        {\"name\": \"bert-large-uncased-imdb\", \"use_prompt\": False}\n",
    "    ]\n",
    "    \n",
    "    for model_config in models:\n",
    "        result = evaluate_model_accuracy_latency(\n",
    "            model_config[\"name\"], \n",
    "            use_prompt=model_config[\"use_prompt\"],\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # Return as DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation with 100 samples\n",
    "results_df = evaluate_all_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "use_prompt",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_latency",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "3eed7dea-a1bd-4aa2-a970-4d6273fe11c6",
       "rows": [
        [
         "0",
         "distilbert-base-uncased",
         "True",
         "0.39",
         "9.116962194442749"
        ],
        [
         "1",
         "distilbert-base-uncased-imdb",
         "False",
         "0.75",
         "7.243594646453857"
        ],
        [
         "2",
         "bert-large-uncased",
         "True",
         "0.43",
         "52.32643175125122"
        ],
        [
         "3",
         "bert-large-uncased-imdb",
         "False",
         "0.8",
         "48.77883958816528"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>use_prompt</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>total_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased</td>\n",
       "      <td>True</td>\n",
       "      <td>0.39</td>\n",
       "      <td>9.116962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased-imdb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7.243595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert-large-uncased</td>\n",
       "      <td>True</td>\n",
       "      <td>0.43</td>\n",
       "      <td>52.326432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-large-uncased-imdb</td>\n",
       "      <td>False</td>\n",
       "      <td>0.80</td>\n",
       "      <td>48.778840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  use_prompt  accuracy  total_latency\n",
       "0       distilbert-base-uncased        True      0.39       9.116962\n",
       "1  distilbert-base-uncased-imdb       False      0.75       7.243595\n",
       "2            bert-large-uncased        True      0.43      52.326432\n",
       "3       bert-large-uncased-imdb       False      0.80      48.778840"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.6410256410256405"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the slm is 6.7x faster than the llm\n",
    "# the LLM is 5% more accurate than the SLM overall\n",
    "\n",
    "# What about the cost?\n",
    "# How much do you use your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
